{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from utils import set_seed, get_data_loaders, get_idx_for_interested_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running DT-etaphi on cpu\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(5)\n",
    "set_seed(42)\n",
    "\n",
    "setting = 'DT_etaphi'\n",
    "config = yaml.safe_load(Path(f'./configs/{setting}.yml').open('r'))\n",
    "print(f'[INFO] Running {setting} on cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Splits]\n",
      "    train: 327108. # pos: 54518, # neg: 272590. Pos:Neg: 0.200\n",
      "    valid: 70092. # pos: 11682, # neg: 58410. Pos:Neg: 0.200\n",
      "    test: 180283. # pos: 11683, # neg: 168600. Pos:Neg: 0.069\n"
     ]
    }
   ],
   "source": [
    "# Load data by using the config. Here the batch_size is not used, so it can be set to any value.\n",
    "data_loaders, x_dim, edge_attr_dim, dataset = get_data_loaders(setting, config['data'], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(data, n_bins, type):\n",
    "    assert type in ['equal_width', 'equal_depth']\n",
    "    if type == 'equal_width':\n",
    "        bins = pd.cut(pd.Series(data), n_bins, labels=False)\n",
    "    else:\n",
    "        bins = pd.qcut(data, n_bins, labels=False)\n",
    "    return bins\n",
    "\n",
    "def bining(x, n_bins, bin_type):\n",
    "    eta_bin = get_bins(x[:, 0], n_bins, bin_type)\n",
    "    phi_bin = get_bins(x[:, 1], n_bins, bin_type)\n",
    "    eta_phi_bin = list(zip(eta_bin, phi_bin))\n",
    "    two_d_bins = sorted(list(set(eta_phi_bin)))\n",
    "    two_d_bins = {each: idx for idx, each in enumerate(two_d_bins)}\n",
    "    assert len(two_d_bins) == n_bins ** 2\n",
    "    return torch.tensor([two_d_bins[each] for each in eta_phi_bin]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "bin_type = 'equal_depth'\n",
    "y = np.array(dataset.data.y).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577483/577483 [00:43<00:00, 13202.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# genereate the bins and count hits in each bin as the features for the decision tree\n",
    "dataset.data.x = bining(dataset.data.x, n_bins, bin_type)\n",
    "x = []\n",
    "for data in tqdm(dataset):\n",
    "    assert data.x.shape[1] == 1\n",
    "    counts = data.x.unique(return_counts=True)\n",
    "    new_x = torch.zeros(n_bins ** 2)\n",
    "    new_x[counts[0]] = counts[1].float()\n",
    "    x.append(new_x)\n",
    "x = np.stack(x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((327108, 100), (70092, 100), (180283, 100))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the training data in case later we want to use other traditional models\n",
    "train_idx = np.array(dataset.idx_split['train'])\n",
    "np.random.shuffle(train_idx)\n",
    "\n",
    "train_x, train_y = x[train_idx], y[train_idx]\n",
    "valid_x, valid_y = x[dataset.idx_split['valid']], y[dataset.idx_split['valid']]\n",
    "test_x, test_y = x[dataset.idx_split['test']], y[dataset.idx_split['test']]\n",
    "train_x.shape, valid_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 100\n",
    "n_estimators = 100\n",
    "# clf = tree.DecisionTreeClassifier(max_depth=50)\n",
    "clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=8, max_depth=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_auc(x, clf_labels, clf):\n",
    "    clf_probs = clf.predict_proba(x)[:, 1]\n",
    "    clf_labels = clf_labels.reshape(-1)\n",
    "\n",
    "    auroc = metrics.roc_auc_score(clf_labels, clf_probs)\n",
    "    partial_auroc = metrics.roc_auc_score(clf_labels, clf_probs, max_fpr=0.001)\n",
    "    fpr, recall, thres = metrics.roc_curve(clf_labels, clf_probs)\n",
    "    indices = get_idx_for_interested_fpr(fpr, [0.001, 0.001/10])\n",
    "    return auroc, recall[indices][0], recall[indices][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: auroc: 0.9997, recall@30khz: 0.9988, recall@3khz: 0.9972\n",
      "Valid: auroc: 0.9861, recall@30khz: 0.7998, recall@3khz: 0.6392\n",
      "Test:  auroc: 0.9850, recall@30khz: 0.7929, recall@3khz: 0.5877\n"
     ]
    }
   ],
   "source": [
    "auroc, recall_30khz, recall_3khz = eval_auc(train_x, train_y, clf)\n",
    "print(f'Train: auroc: {auroc:.4f}, recall@30khz: {recall_30khz:.4f}, recall@3khz: {recall_3khz:.4f}')\n",
    "\n",
    "auroc, recall_30khz, recall_3khz = eval_auc(valid_x, valid_y, clf)\n",
    "print(f'Valid: auroc: {auroc:.4f}, recall@30khz: {recall_30khz:.4f}, recall@3khz: {recall_3khz:.4f}')\n",
    "\n",
    "auroc, recall_30khz, recall_3khz = eval_auc(test_x, test_y, clf)\n",
    "print(f'Test:  auroc: {auroc:.4f}, recall@30khz: {recall_30khz:.4f}, recall@3khz: {recall_3khz:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45135e56c447e54bdb1e1c0bb84ad415b64665bf9f8fb7efb25fee4420f52c63"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tau3mu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
